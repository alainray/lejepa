{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LeJEPA completo (SICReg + JEPA) en MPI3D / dSprites\n",
        "\n",
        "Este notebook entrena **LeJEPA completo** usando el esquema de `MINIMAL.md` y el loader reducido de `dataset.py` para los datasets **MPI3D** y **dSprites**.  \n",
        "Luego entrena *probes* lineales sobre las representaciones para evaluar cada factor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.ops import MLP\n",
        "from torchvision.transforms import v2\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dataset import IDGBenchmarkDataset, IDGDatasetName, IDGSplitName"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    data_root: str = \"./data\"\n",
        "    dataset: IDGDatasetName = \"dsprites\"  # \"dsprites\" o \"mpi3d\"\n",
        "    split: IDGSplitName = \"random\"\n",
        "    backbone: str = \"vit_small_patch8_224\"\n",
        "    img_size: int = 64\n",
        "    proj_dim: int = 128\n",
        "    views: int = 2\n",
        "    epochs: int = 100\n",
        "    batch_size: int = 256\n",
        "    lr: float = 2e-3\n",
        "    weight_decay: float = 5e-2\n",
        "    lamb: float = 0.02\n",
        "    num_workers: int = 8\n",
        "    device: str = \"cuda\"\n",
        "    amp: bool = True\n",
        "    probe_epochs: int = 50\n",
        "    probe_lr: float = 1e-3\n",
        "    probe_weight_decay: float = 1e-6\n",
        "    probe_batch_size: int = 256\n",
        "\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset + augmentaciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MultiViewIDGDataset(Dataset):\n",
        "    def __init__(self, root, dataset, split, mode, views, transform):\n",
        "        self.views = views\n",
        "        self.transform = transform\n",
        "        self.ds = IDGBenchmarkDataset(\n",
        "            root=root,\n",
        "            dataset=dataset,\n",
        "            split=split,\n",
        "            mode=mode,\n",
        "            image_as_float=True,\n",
        "            latents_dtype=torch.long,\n",
        "            transform=None,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, latents, _ = self.ds[idx]\n",
        "        views = torch.stack([self.transform(img) for _ in range(self.views)])\n",
        "        return views, latents\n",
        "\n",
        "\n",
        "def build_transforms(img_size: int, train: bool):\n",
        "    if train:\n",
        "        return v2.Compose(\n",
        "            [\n",
        "                v2.RandomResizedCrop(img_size, scale=(0.6, 1.0)),\n",
        "                v2.RandomHorizontalFlip(),\n",
        "                v2.RandomApply([v2.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.3),\n",
        "            ]\n",
        "        )\n",
        "    return v2.Compose([v2.Resize(img_size), v2.CenterCrop(img_size)])\n",
        "\n",
        "train_transform = build_transforms(cfg.img_size, train=True)\n",
        "test_transform = build_transforms(cfg.img_size, train=False)\n",
        "\n",
        "train_ds = MultiViewIDGDataset(\n",
        "    root=cfg.data_root,\n",
        "    dataset=cfg.dataset,\n",
        "    split=cfg.split,\n",
        "    mode=\"train\",\n",
        "    views=cfg.views,\n",
        "    transform=train_transform,\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=cfg.num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=cfg.num_workers > 0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Modelo LeJEPA completo (SICReg + pérdida JEPA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SICReg(nn.Module):\n",
        "    def __init__(self, knots: int = 17, t_max: float = 3.0):\n",
        "        super().__init__()\n",
        "        t = torch.linspace(0, t_max, knots, dtype=torch.float32)\n",
        "        dt = t_max / (knots - 1)\n",
        "        weights = torch.full((knots,), 2 * dt, dtype=torch.float32)\n",
        "        weights[[0, -1]] = dt\n",
        "        window = torch.exp(-t.square() / 2.0)\n",
        "        self.register_buffer(\"t\", t)\n",
        "        self.register_buffer(\"phi\", window)\n",
        "        self.register_buffer(\"weights\", weights * window)\n",
        "\n",
        "    def forward(self, proj: torch.Tensor) -> torch.Tensor:\n",
        "        proj_dim = proj.size(-1)\n",
        "        sketch_dim = min(256, proj_dim)\n",
        "        A = torch.randn(proj_dim, sketch_dim, device=proj.device)\n",
        "        A = A.div_(A.norm(p=2, dim=0))\n",
        "        x_t = (proj @ A).unsqueeze(-1) * self.t\n",
        "        err = (x_t.cos().mean(-3) - self.phi).square() + x_t.sin().mean(-3).square()\n",
        "        statistic = (err @ self.weights) * proj.size(-2)\n",
        "        return statistic.mean()\n",
        "\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, backbone: str, img_size: int, proj_dim: int):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            backbone,\n",
        "            pretrained=False,\n",
        "            num_classes=512,\n",
        "            drop_path_rate=0.1,\n",
        "            img_size=img_size,\n",
        "        )\n",
        "        self.proj = MLP(512, [2048, 2048, proj_dim], norm_layer=nn.BatchNorm1d)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n, v = x.shape[:2]\n",
        "        emb = self.backbone(x.flatten(0, 1))\n",
        "        proj = self.proj(emb).reshape(n, v, -1).transpose(0, 1)\n",
        "        return emb, proj\n",
        "\n",
        "\n",
        "device = torch.device(cfg.device)\n",
        "model = ViTEncoder(cfg.backbone, cfg.img_size, cfg.proj_dim).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "warmup_steps = max(1, len(train_loader))\n",
        "total_steps = len(train_loader) * cfg.epochs\n",
        "scheduler = SequentialLR(\n",
        "    optimizer,\n",
        "    schedulers=[\n",
        "        LinearLR(optimizer, start_factor=0.01, total_iters=warmup_steps),\n",
        "        CosineAnnealingLR(optimizer, T_max=max(1, total_steps - warmup_steps), eta_min=1e-4),\n",
        "    ],\n",
        "    milestones=[warmup_steps],\n",
        ")\n",
        "scaler = GradScaler(enabled=cfg.amp)\n",
        "sicreg = SICReg().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Entrenamiento LeJEPA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for epoch in range(cfg.epochs):\n",
        "    model.train()\n",
        "    inv_loss_sum = 0.0\n",
        "    sicreg_sum = 0.0\n",
        "    lejepa_sum = 0.0\n",
        "    progress = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{cfg.epochs}\")\n",
        "    for views, _ in progress:\n",
        "        views = views.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(device_type=device.type, dtype=torch.bfloat16, enabled=cfg.amp):\n",
        "            _, proj = model(views)\n",
        "            inv_loss = (proj.mean(0) - proj).square().mean()\n",
        "            sicreg_loss = sicreg(proj)\n",
        "            lejepa_loss = sicreg_loss * cfg.lamb + inv_loss * (1 - cfg.lamb)\n",
        "        scaler.scale(lejepa_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        inv_loss_sum += inv_loss.item()\n",
        "        sicreg_sum += sicreg_loss.item()\n",
        "        lejepa_sum += lejepa_loss.item()\n",
        "        step = progress.n + 1\n",
        "        progress.set_postfix(\n",
        "            inv=inv_loss_sum / step,\n",
        "            sicreg=sicreg_sum / step,\n",
        "            lejepa=lejepa_sum / step,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Entrenar probes por factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "probe_train_ds = IDGBenchmarkDataset(\n",
        "    root=cfg.data_root,\n",
        "    dataset=cfg.dataset,\n",
        "    split=cfg.split,\n",
        "    mode=\"train\",\n",
        "    image_as_float=True,\n",
        "    latents_dtype=torch.long,\n",
        "    transform=test_transform,\n",
        ")\n",
        "probe_test_ds = IDGBenchmarkDataset(\n",
        "    root=cfg.data_root,\n",
        "    dataset=cfg.dataset,\n",
        "    split=cfg.split,\n",
        "    mode=\"test\",\n",
        "    image_as_float=True,\n",
        "    latents_dtype=torch.long,\n",
        "    transform=test_transform,\n",
        ")\n",
        "\n",
        "probe_train_loader = DataLoader(\n",
        "    probe_train_ds,\n",
        "    batch_size=cfg.probe_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=cfg.num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=cfg.num_workers > 0,\n",
        ")\n",
        "probe_test_loader = DataLoader(\n",
        "    probe_test_ds,\n",
        "    batch_size=cfg.probe_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=cfg.num_workers > 0,\n",
        ")\n",
        "\n",
        "labels = probe_train_ds._labels\n",
        "if labels.ndim == 1:\n",
        "    labels = labels[:, None]\n",
        "num_classes = [int(labels[:, i].max()) + 1 for i in range(labels.shape[1])]\n",
        "\n",
        "probes = nn.ModuleList([nn.Linear(512, n) for n in num_classes]).to(device)\n",
        "probe_opt = torch.optim.AdamW(probes.parameters(), lr=cfg.probe_lr, weight_decay=cfg.probe_weight_decay)\n",
        "\n",
        "for epoch in range(cfg.probe_epochs):\n",
        "    probes.train()\n",
        "    running = 0.0\n",
        "    progress = tqdm(probe_train_loader, desc=f\"Probe {epoch + 1}/{cfg.probe_epochs}\")\n",
        "    for imgs, latents, _ in progress:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        latents = latents.to(device, non_blocking=True)\n",
        "        with torch.no_grad():\n",
        "            emb, _ = model(imgs[:, None])\n",
        "        losses = [F.cross_entropy(head(emb), latents[:, i]) for i, head in enumerate(probes)]\n",
        "        loss = sum(losses)\n",
        "        probe_opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        probe_opt.step()\n",
        "        running += loss.item()\n",
        "        progress.set_postfix(loss=running / (progress.n + 1))\n",
        "\n",
        "probes.eval()\n",
        "correct = [0 for _ in num_classes]\n",
        "total = 0\n",
        "with torch.inference_mode():\n",
        "    for imgs, latents, _ in tqdm(probe_test_loader, desc=\"Eval probes\"):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        latents = latents.to(device, non_blocking=True)\n",
        "        emb, _ = model(imgs[:, None])\n",
        "        for i, head in enumerate(probes):\n",
        "            pred = head(emb).argmax(dim=1)\n",
        "            correct[i] += (pred == latents[:, i]).sum().item()\n",
        "        total += latents.size(0)\n",
        "\n",
        "for i, c in enumerate(correct):\n",
        "    print(f\"Factor {i}: {c / total:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}