{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d471a5df",
      "metadata": {},
      "source": [
        "# Comparaci\u00f3n de entrenamiento est\u00e1ndar vs SICReg en datasets de torchvision\n",
        "\n",
        "Este notebook entrena una **ResNet18** en varios datasets de `torchvision` (MNIST, CIFAR10, CIFAR100) con:\n",
        "\n",
        "1. **Entrenamiento est\u00e1ndar** (cross-entropy).\n",
        "2. **Entrenamiento est\u00e1ndar + p\u00e9rdida SICReg**.\n",
        "\n",
        "Al final se genera una tabla con los resultados de generalizaci\u00f3n y gr\u00e1ficos con las curvas de aprendizaje para comparar ambos m\u00e9todos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b78490d",
      "metadata": {},
      "source": [
        "## 1. Imports y configuraci\u00f3n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d1b769c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Reproducibilidad y registro de experimentos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(SEED)\n",
        "\n",
        "RUN_DIR = Path('experiments')\n",
        "CHECKPOINT_DIR = RUN_DIR / 'checkpoints'\n",
        "LOG_PATH = RUN_DIR / 'sicreg_runs.json'\n",
        "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_runs() -> Dict[str, dict]:\n",
        "    if LOG_PATH.exists():\n",
        "        return json.loads(LOG_PATH.read_text())\n",
        "    return {}\n",
        "\n",
        "def save_runs(runs: Dict[str, dict]) -> None:\n",
        "    LOG_PATH.write_text(json.dumps(runs, indent=2))\n",
        "\n",
        "class EarlyStoppingScheduler:\n",
        "    def __init__(self, patience: int = 5):\n",
        "        self.patience = patience\n",
        "        self.best = None\n",
        "        self.epochs_without_improve = 0\n",
        "\n",
        "    def should_stop(self, metric: float) -> bool:\n",
        "        if self.best is None or metric > self.best:\n",
        "            self.best = metric\n",
        "            self.epochs_without_improve = 0\n",
        "            return False\n",
        "        self.epochs_without_improve += 1\n",
        "        return self.epochs_without_improve >= self.patience\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83411785",
      "metadata": {},
      "source": [
        "## 2. Definici\u00f3n de la p\u00e9rdida SICReg\n",
        "\n",
        "La implementaci\u00f3n sigue el esquema del ejemplo m\u00ednimo (SIGReg) pero usando el nombre **SICReg** para mantener la convenci\u00f3n del enunciado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3e93688d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SICReg(nn.Module):\n",
        "    def __init__(self, knots: int = 17):\n",
        "        super().__init__()\n",
        "        t = torch.linspace(0, 3, knots, dtype=torch.float32)\n",
        "        dt = 3 / (knots - 1)\n",
        "        weights = torch.full((knots,), 2 * dt, dtype=torch.float32)\n",
        "        weights[[0, -1]] = dt\n",
        "        window = torch.exp(-t.square() / 2.0)\n",
        "        self.register_buffer('t', t)\n",
        "        self.register_buffer('phi', window)\n",
        "        self.register_buffer('weights', weights * window)\n",
        "\n",
        "    def forward(self, proj: torch.Tensor) -> torch.Tensor:\n",
        "        # proj: [V, B, D]\n",
        "        A = torch.randn(proj.size(-1), 256, device=proj.device)\n",
        "        A = A.div_(A.norm(p=2, dim=0))\n",
        "        x_t = (proj @ A).unsqueeze(-1) * self.t\n",
        "        err = (x_t.cos().mean(-3) - self.phi).square() + x_t.sin().mean(-3).square()\n",
        "        statistic = (err @ self.weights) * proj.size(-2)\n",
        "        return statistic.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "481322ef",
      "metadata": {},
      "source": [
        "## 3. Modelo ResNet18 con cabeza de proyecci\u00f3n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5c2a9d5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResNet18WithHead(nn.Module):\n",
        "    def __init__(self, num_classes: int, proj_dim: int = 128):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet18(weights=None)\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, proj_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        logits = self.classifier(features)\n",
        "        proj = self.projector(features)\n",
        "        return logits, proj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4add366d",
      "metadata": {},
      "source": [
        "## 4. Datasets y DataLoaders\n",
        "\n",
        "Se construyen transformaciones espec\u00edficas para cada dataset. Para **MNIST** se replica el canal a 3 para adaptarlo a ResNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "851b17aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DatasetConfig:\n",
        "    name: str\n",
        "    dataset_cls: type\n",
        "    num_classes: int\n",
        "    image_size: int\n",
        "\n",
        "DATASETS = [\n",
        "    DatasetConfig('MNIST', datasets.MNIST, 10, 28),\n",
        "    DatasetConfig('CIFAR10', datasets.CIFAR10, 10, 32),\n",
        "    DatasetConfig('CIFAR100', datasets.CIFAR100, 100, 32),\n",
        "]\n",
        "\n",
        "def build_transforms(image_size: int, is_train: bool, grayscale: bool = False):\n",
        "    if is_train:\n",
        "        tfm = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "        ])\n",
        "    else:\n",
        "        tfm = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "        ])\n",
        "\n",
        "    to_tensor = [transforms.ToTensor()]\n",
        "    if grayscale:\n",
        "        to_tensor.append(transforms.Lambda(lambda x: x.repeat(3, 1, 1)))\n",
        "    return transforms.Compose([tfm] + to_tensor)\n",
        "\n",
        "class MultiViewDataset(Dataset):\n",
        "    def __init__(self, base_dataset: Dataset, views: int, transform):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.views = views\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.base_dataset[idx]\n",
        "        return torch.stack([self.transform(x) for _ in range(self.views)]), y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d79a66d8",
      "metadata": {},
      "source": [
        "## 5. Funciones de entrenamiento y evaluaci\u00f3n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ee590cd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits, _ = model(x)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def train_one_epoch_sicreg(model, loader, optimizer, sicreg: SICReg, lamb: float):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_cls_loss = 0.0\n",
        "    running_sicreg_loss = 0.0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        logits, proj = model(x)\n",
        "        cls_loss = F.cross_entropy(logits, y)\n",
        "        sicreg_loss = sicreg(proj)\n",
        "        loss = cls_loss + lamb * sicreg_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        running_cls_loss += cls_loss.item() * y.size(0)\n",
        "        running_sicreg_loss += sicreg_loss.item() * y.size(0)\n",
        "    return running_loss / len(loader.dataset), running_cls_loss / len(loader.dataset), running_sicreg_loss / len(loader.dataset)\n",
        "\n",
        "def train_one_epoch_baseline(model, loader, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        logits, _ = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b16da46",
      "metadata": {},
      "source": [
        "## 6. Ejecutar experimentos\n",
        "\n",
        "Ajust\u00e1 `EPOCHS`, `BATCH_SIZE` o `LAMB` seg\u00fan recursos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d45dade5",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "824169f43946400bb142c40f4963c8a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Datasets:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Dataset: MNIST\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "485aba63368d47679300fb944e42fd70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training with SICReg:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [SICReg]  Epoch 1/10 - loss: 0.7582 - acc: 0.9600 - cls: 0.2600 - sicreg: 9.9624\n",
            "  [SICReg]  Epoch 2/10 - loss: 0.3064 - acc: 0.9639 - cls: 0.1018 - sicreg: 4.0934\n",
            "  [SICReg]  Epoch 3/10 - loss: 0.2407 - acc: 0.9768 - cls: 0.0808 - sicreg: 3.1983\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 256\n",
        "LR = 1e-3\n",
        "LAMB = 0.05\n",
        "EARLY_STOP_PATIENCE = 5\n",
        "\n",
        "results: Dict[str, Dict[str, List[float]]] = {}\n",
        "summary_rows = []\n",
        "runs = load_runs()\n",
        "\n",
        "for cfg in tqdm(DATASETS, desc='Datasets'):\n",
        "    print(f'==> Dataset: {cfg.name}')\n",
        "    grayscale = cfg.name == 'MNIST'\n",
        "    train_tfm = build_transforms(cfg.image_size, is_train=True, grayscale=grayscale)\n",
        "    test_tfm = build_transforms(cfg.image_size, is_train=False, grayscale=grayscale)\n",
        "\n",
        "    train_base = cfg.dataset_cls(root='data', train=True, download=True, transform=train_tfm)\n",
        "    train_raw = cfg.dataset_cls(root='data', train=True, download=True, transform=None)\n",
        "    test_base = cfg.dataset_cls(root='data', train=False, download=True, transform=test_tfm)\n",
        "\n",
        "    baseline_loader = DataLoader(train_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    test_loader = DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    mv_dataset = MultiViewDataset(train_raw, views=2, transform=train_tfm)\n",
        "    sicreg_loader = DataLoader(mv_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "    run_key = f'{cfg.name}-seed{SEED}-epochs{EPOCHS}-lr{LR}-lamb{LAMB}-bs{BATCH_SIZE}'\n",
        "    run_state = runs.get(run_key)\n",
        "\n",
        "    if run_state and run_state.get('status') == 'completed':\n",
        "        print(f'  [SICReg] Resultado ya completado, se omite: {run_key}')\n",
        "        results[cfg.name] = run_state['metrics']\n",
        "        summary_rows.append({\n",
        "            'Dataset': cfg.name,\n",
        "            'Baseline Acc (last)': run_state['metrics']['baseline_acc'][-1] if run_state['metrics']['baseline_acc'] else None,\n",
        "            'SICReg Acc (last)': run_state['metrics']['sicreg_acc'][-1] if run_state['metrics']['sicreg_acc'] else None,\n",
        "            'Baseline Acc (best)': max(run_state['metrics']['baseline_acc']) if run_state['metrics']['baseline_acc'] else None,\n",
        "            'SICReg Acc (best)': max(run_state['metrics']['sicreg_acc']) if run_state['metrics']['sicreg_acc'] else None,\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # Baseline training\n",
        "    baseline_model = ResNet18WithHead(cfg.num_classes).to(device)\n",
        "    baseline_opt = torch.optim.Adam(baseline_model.parameters(), lr=LR)\n",
        "    baseline_losses = []\n",
        "    baseline_accs = []\n",
        "    baseline_early_stopper = EarlyStoppingScheduler(patience=EARLY_STOP_PATIENCE)\n",
        "    baseline_checkpoint_path = CHECKPOINT_DIR / f'{run_key}-baseline.pt'\n",
        "    baseline_start_epoch = 1\n",
        "\n",
        "    if run_state:\n",
        "        baseline_losses = run_state['metrics'].get('baseline_loss', [])\n",
        "        baseline_accs = run_state['metrics'].get('baseline_acc', [])\n",
        "        baseline_early_stopper.best = run_state.get('baseline_best_acc')\n",
        "        baseline_early_stopper.epochs_without_improve = run_state.get('baseline_epochs_without_improve', 0)\n",
        "        if baseline_checkpoint_path.exists():\n",
        "            checkpoint = torch.load(baseline_checkpoint_path, map_location=device)\n",
        "            baseline_model.load_state_dict(checkpoint['model_state'])\n",
        "            baseline_opt.load_state_dict(checkpoint['optimizer_state'])\n",
        "            baseline_start_epoch = checkpoint['epoch'] + 1\n",
        "            print(f'  [Baseline] Reanudando desde \u00e9poca {baseline_start_epoch}')\n",
        "\n",
        "    if run_state and run_state.get('baseline_status') == 'completed':\n",
        "        print(f'  [Baseline] Resultado ya completado, se omite: {run_key}')\n",
        "    else:\n",
        "        for epoch in tqdm(range(baseline_start_epoch, EPOCHS + 1), desc='Training Baseline'):\n",
        "            loss = train_one_epoch_baseline(baseline_model, baseline_loader, baseline_opt)\n",
        "            acc = evaluate(baseline_model, test_loader)\n",
        "            baseline_losses.append(loss)\n",
        "            baseline_accs.append(acc)\n",
        "            print(f'  [Baseline] Epoch {epoch}/{EPOCHS} - loss: {loss:.4f} - acc: {acc:.4f}')\n",
        "\n",
        "            should_stop = baseline_early_stopper.should_stop(acc)\n",
        "            runs[run_key] = {\n",
        "                'status': 'in_progress',\n",
        "                'epoch': run_state.get('epoch', 0) if run_state else 0,\n",
        "                'baseline_epoch': epoch,\n",
        "                'baseline_best_acc': baseline_early_stopper.best,\n",
        "                'baseline_epochs_without_improve': baseline_early_stopper.epochs_without_improve,\n",
        "                'baseline_status': 'in_progress',\n",
        "                'metrics': {\n",
        "                    'baseline_loss': baseline_losses,\n",
        "                    'baseline_acc': baseline_accs,\n",
        "                    'sicreg_total_loss': run_state['metrics'].get('sicreg_total_loss', []) if run_state else [],\n",
        "                    'sicreg_cls_loss': run_state['metrics'].get('sicreg_cls_loss', []) if run_state else [],\n",
        "                    'sicreg_loss': run_state['metrics'].get('sicreg_loss', []) if run_state else [],\n",
        "                    'sicreg_acc': run_state['metrics'].get('sicreg_acc', []) if run_state else [],\n",
        "                },\n",
        "            }\n",
        "            save_runs(runs)\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state': baseline_model.state_dict(),\n",
        "                'optimizer_state': baseline_opt.state_dict(),\n",
        "            }, baseline_checkpoint_path)\n",
        "            if should_stop:\n",
        "                print(f'  [Baseline] Early stopping activado en \u00e9poca {epoch}')\n",
        "                break\n",
        "\n",
        "        runs[run_key] = {\n",
        "            'status': 'in_progress',\n",
        "            'epoch': run_state.get('epoch', 0) if run_state else 0,\n",
        "            'baseline_epoch': len(baseline_accs),\n",
        "            'baseline_best_acc': baseline_early_stopper.best,\n",
        "            'baseline_epochs_without_improve': baseline_early_stopper.epochs_without_improve,\n",
        "            'baseline_status': 'completed',\n",
        "            'metrics': {\n",
        "                'baseline_loss': baseline_losses,\n",
        "                'baseline_acc': baseline_accs,\n",
        "                'sicreg_total_loss': run_state['metrics'].get('sicreg_total_loss', []) if run_state else [],\n",
        "                'sicreg_cls_loss': run_state['metrics'].get('sicreg_cls_loss', []) if run_state else [],\n",
        "                'sicreg_loss': run_state['metrics'].get('sicreg_loss', []) if run_state else [],\n",
        "                'sicreg_acc': run_state['metrics'].get('sicreg_acc', []) if run_state else [],\n",
        "            },\n",
        "        }\n",
        "        save_runs(runs)\n",
        "\n",
        "    # SICReg training\n",
        "    sicreg_model = ResNet18WithHead(cfg.num_classes).to(device)\n",
        "    sicreg_opt = torch.optim.Adam(sicreg_model.parameters(), lr=LR)\n",
        "    sicreg_total_losses = []\n",
        "    sicreg_cls_losses = []\n",
        "    sicreg_losses = []\n",
        "    sicreg_accs = []\n",
        "    sicreg_loss_fn = SICReg().to(device)\n",
        "    early_stopper = EarlyStoppingScheduler(patience=EARLY_STOP_PATIENCE)\n",
        "\n",
        "    checkpoint_path = CHECKPOINT_DIR / f'{run_key}.pt'\n",
        "    start_epoch = 1\n",
        "    if run_state:\n",
        "        sicreg_total_losses = run_state['metrics'].get('sicreg_total_loss', [])\n",
        "        sicreg_cls_losses = run_state['metrics'].get('sicreg_cls_loss', [])\n",
        "        sicreg_losses = run_state['metrics'].get('sicreg_loss', [])\n",
        "        sicreg_accs = run_state['metrics'].get('sicreg_acc', [])\n",
        "        early_stopper.best = run_state.get('best_acc')\n",
        "        early_stopper.epochs_without_improve = run_state.get('epochs_without_improve', 0)\n",
        "        if checkpoint_path.exists():\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "            sicreg_model.load_state_dict(checkpoint['model_state'])\n",
        "            sicreg_opt.load_state_dict(checkpoint['optimizer_state'])\n",
        "            start_epoch = checkpoint['epoch'] + 1\n",
        "            print(f'  [SICReg] Reanudando desde \u00e9poca {start_epoch}')\n",
        "\n",
        "    for epoch in tqdm(range(start_epoch, EPOCHS + 1), desc='Training with SICReg'):\n",
        "        loss, cls_loss, sicreg_loss = train_one_epoch_sicreg(sicreg_model, baseline_loader, sicreg_opt, sicreg_loss_fn, LAMB)\n",
        "        acc = evaluate(sicreg_model, test_loader)\n",
        "        sicreg_total_losses.append(loss)\n",
        "        sicreg_cls_losses.append(cls_loss)\n",
        "        sicreg_losses.append(sicreg_loss)\n",
        "        sicreg_accs.append(acc)\n",
        "        print(f'  [SICReg]  Epoch {epoch}/{EPOCHS} - loss: {loss:.4f} - acc: {acc:.4f} - cls: {cls_loss:.4f} - sicreg: {sicreg_loss:.4f}')\n",
        "\n",
        "        should_stop = early_stopper.should_stop(acc)\n",
        "        runs[run_key] = {\n",
        "            'status': 'in_progress',\n",
        "            'epoch': epoch,\n",
        "            'best_acc': early_stopper.best,\n",
        "            'epochs_without_improve': early_stopper.epochs_without_improve,\n",
        "            'baseline_epoch': len(baseline_accs),\n",
        "            'baseline_best_acc': baseline_early_stopper.best,\n",
        "            'baseline_epochs_without_improve': baseline_early_stopper.epochs_without_improve,\n",
        "            'baseline_status': run_state.get('baseline_status', 'completed') if run_state else 'completed',\n",
        "            'metrics': {\n",
        "                'baseline_loss': baseline_losses,\n",
        "                'baseline_acc': baseline_accs,\n",
        "                'sicreg_total_loss': sicreg_total_losses,\n",
        "                'sicreg_cls_loss': sicreg_cls_losses,\n",
        "                'sicreg_loss': sicreg_losses,\n",
        "                'sicreg_acc': sicreg_accs,\n",
        "            },\n",
        "        }\n",
        "        save_runs(runs)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state': sicreg_model.state_dict(),\n",
        "            'optimizer_state': sicreg_opt.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "        if should_stop:\n",
        "            print(f'  [SICReg] Early stopping activado en \u00e9poca {epoch}')\n",
        "            break\n",
        "\n",
        "    runs[run_key] = {\n",
        "        'status': 'completed',\n",
        "        'epoch': len(sicreg_accs),\n",
        "        'best_acc': early_stopper.best,\n",
        "        'epochs_without_improve': early_stopper.epochs_without_improve,\n",
        "        'baseline_epoch': len(baseline_accs),\n",
        "        'baseline_best_acc': baseline_early_stopper.best,\n",
        "        'baseline_epochs_without_improve': baseline_early_stopper.epochs_without_improve,\n",
        "        'baseline_status': 'completed',\n",
        "        'metrics': {\n",
        "            'baseline_loss': baseline_losses,\n",
        "            'baseline_acc': baseline_accs,\n",
        "            'sicreg_total_loss': sicreg_total_losses,\n",
        "            'sicreg_cls_loss': sicreg_cls_losses,\n",
        "            'sicreg_loss': sicreg_losses,\n",
        "            'sicreg_acc': sicreg_accs,\n",
        "        },\n",
        "    }\n",
        "    save_runs(runs)\n",
        "\n",
        "    results[cfg.name] = {\n",
        "        'baseline_loss': baseline_losses,\n",
        "        'baseline_acc': baseline_accs,\n",
        "        'sicreg_loss': sicreg_losses,\n",
        "        'sicreg_acc': sicreg_accs,\n",
        "    }\n",
        "    summary_rows.append({\n",
        "        'Dataset': cfg.name,\n",
        "        'Baseline Acc (last)': baseline_accs[-1] if baseline_accs else None,\n",
        "        'SICReg Acc (last)': sicreg_accs[-1] if sicreg_accs else None,\n",
        "        'Baseline Acc (best)': max(baseline_accs) if baseline_accs else None,\n",
        "        'SICReg Acc (best)': max(sicreg_accs) if sicreg_accs else None,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "320fac47",
      "metadata": {},
      "source": [
        "## 7. Tabla comparativa de generalizaci\u00f3n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4977424d",
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ac188c",
      "metadata": {},
      "source": [
        "## 8. Curvas de aprendizaje por dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b1f4a51",
      "metadata": {},
      "outputs": [],
      "source": [
        "for dataset_name, data in results.items():\n",
        "    epochs = list(range(1, len(data['baseline_acc']) + 1))\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(epochs, data['baseline_acc'], label='Baseline')\n",
        "    plt.plot(epochs, data['sicreg_acc'], label='SICReg')\n",
        "    plt.title(f'Accuracy vs Epochs - {dataset_name}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2dc3b14",
      "metadata": {},
      "source": [
        "---\n",
        "**Notas:**\n",
        "- Para resultados m\u00e1s robustos aument\u00e1 `EPOCHS` y consider\u00e1 usar un scheduler.\n",
        "- Pod\u00e9s agregar m\u00e1s datasets de `torchvision` extendiendo la lista `DATASETS`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}