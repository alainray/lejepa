{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Análisis de experimentos y métricas\n\nEste notebook resume y visualiza los resultados de los experimentos almacenados en un directorio (por defecto `experiments/`).\n\n**Estructura esperada (por ejecución):**\n\n- `metrics.jsonl`: log de eventos con métricas por época.\n- `summary.json`: resumen final (tiempos, hiperparámetros, métricas).\n- `train_history.json` (opcional): historial de entrenamiento por época.\n\nSi tu estructura difiere, ajusta las rutas y/o los parsers en las siguientes celdas.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from __future__ import annotations\n\nimport json\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Ruta base donde se encuentran los experimentos\nBASE_DIR = Path(\"experiments\")\n\n# Puedes cambiarlo, por ejemplo:\n# BASE_DIR = Path(\"/ruta/a/experimentos\")\n\nBASE_DIR\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def load_jsonl(path: Path) -> List[Dict[str, Any]]:\n    records: List[Dict[str, Any]] = []\n    with path.open(\"r\", encoding=\"utf-8\") as handle:\n        for line in handle:\n            line = line.strip()\n            if not line:\n                continue\n            records.append(json.loads(line))\n    return records\n\n\ndef load_json(path: Path) -> Dict[str, Any]:\n    with path.open(\"r\", encoding=\"utf-8\") as handle:\n        return json.load(handle)\n\n\ndef find_runs(base_dir: Path) -> List[Path]:\n    if not base_dir.exists():\n        return []\n    return sorted({p.parent for p in base_dir.rglob(\"metrics.jsonl\")} | {p.parent for p in base_dir.rglob(\"summary.json\")})\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "runs = find_runs(BASE_DIR)\nprint(f\"Runs encontrados: {len(runs)}\")\n\n# Muestra los primeros 10\nruns[:10]\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def extract_config_from_events(events: List[Dict[str, Any]]) -> Dict[str, Any]:\n    for item in events:\n        if item.get(\"event\") == \"config\":\n            return item\n    return {}\n\n\ndef extract_run_summary(run_dir: Path) -> Dict[str, Any]:\n    summary_path = run_dir / \"summary.json\"\n    metrics_path = run_dir / \"metrics.jsonl\"\n    payload: Dict[str, Any] = {\"run_dir\": str(run_dir)}\n\n    if summary_path.exists():\n        summary = load_json(summary_path)\n        payload.update({\n            \"method\": summary.get(\"method\"),\n            \"seed\": summary.get(\"seed\"),\n            \"model\": summary.get(\"model\"),\n            \"train_time_s\": summary.get(\"train_time_s\"),\n            \"probe_time_s\": summary.get(\"probe_time_s\"),\n            \"train_epochs\": summary.get(\"train_epochs\"),\n            \"probe_epochs\": summary.get(\"probe_epochs\"),\n        })\n        if isinstance(summary.get(\"probe_test\"), dict):\n            for key, value in summary[\"probe_test\"].items():\n                payload[f\"probe_{key}\"] = value\n        if isinstance(summary.get(\"hparams\"), dict):\n            payload.update({f\"hparam_{k}\": v for k, v in summary[\"hparams\"].items()})\n\n    if metrics_path.exists():\n        events = load_jsonl(metrics_path)\n        config_event = extract_config_from_events(events)\n        if config_event:\n            payload.setdefault(\"method\", config_event.get(\"method\"))\n            payload.setdefault(\"seed\", config_event.get(\"seed\"))\n            payload.setdefault(\"model\", config_event.get(\"model\"))\n            hparams = config_event.get(\"hparams\")\n            if isinstance(hparams, dict):\n                payload.update({f\"hparam_{k}\": v for k, v in hparams.items()})\n\n    return payload\n\n\nsummaries = [extract_run_summary(run_dir) for run_dir in runs]\nsummary_df = pd.DataFrame(summaries)\nsummary_df\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Tabla de resultados\n\nFiltra o ordena la tabla para revisar las métricas principales. Puedes modificar `columns` para quedarte con los campos de interés.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "columns = [\n    \"run_dir\",\n    \"method\",\n    \"seed\",\n    \"model\",\n    \"train_time_s\",\n    \"probe_time_s\",\n    \"train_epochs\",\n    \"probe_epochs\",\n] + [col for col in summary_df.columns if col.startswith(\"probe_\")]\n\nsummary_df[columns].sort_values(by=[\"method\", \"seed\"]).reset_index(drop=True)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Visualizar curvas de entrenamiento\n\nSelecciona un `run_dir` y dibuja las métricas registradas por época.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def load_train_history(run_dir: Path) -> pd.DataFrame:\n    history_path = run_dir / \"train_history.json\"\n    if history_path.exists():\n        history = load_json(history_path)\n        return pd.DataFrame(history)\n    # fallback: intenta reconstruir desde metrics.jsonl\n    metrics_path = run_dir / \"metrics.jsonl\"\n    if not metrics_path.exists():\n        return pd.DataFrame()\n    events = load_jsonl(metrics_path)\n    rows = []\n    for item in events:\n        if item.get(\"event\") == \"train_epoch\":\n            metrics = item.get(\"metrics\", {})\n            row = {\"epoch\": item.get(\"epoch\")}\n            row.update(metrics)\n            rows.append(row)\n    return pd.DataFrame(rows)\n\n\nif runs:\n    run_dir = Path(runs[0])\nelse:\n    run_dir = Path(\"/ruta/a/tu/run\")\n\nhistory_df = load_train_history(run_dir)\nhistory_df.head()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if not history_df.empty:\n    metric_cols = [col for col in history_df.columns if col not in {\"epoch\", \"epoch_time_s\"}]\n    fig, axes = plt.subplots(len(metric_cols), 1, figsize=(8, 3 * len(metric_cols)), sharex=True)\n    if len(metric_cols) == 1:\n        axes = [axes]\n    for ax, col in zip(axes, metric_cols):\n        sns.lineplot(data=history_df, x=\"epoch\", y=col, ax=ax)\n        ax.set_title(f\"{col} por época\")\n    plt.tight_layout()\nelse:\n    print(\"No se encontró historial de entrenamiento para este run.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Comparación rápida entre ejecuciones\n\nEjemplo de comparación de una métrica específica (p.ej. `probe_factor_0`). Ajusta `metric_name` según tus datos.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "metric_name = \"probe_factor_0\"\n\nif metric_name in summary_df.columns:\n    fig, ax = plt.subplots(figsize=(8, 4))\n    sns.barplot(data=summary_df, x=\"method\", y=metric_name, ax=ax, errorbar=None)\n    ax.set_title(f\"Comparación de {metric_name}\")\n    ax.set_ylabel(metric_name)\n    plt.tight_layout()\nelse:\n    print(f\"La métrica '{metric_name}' no está disponible en summary_df.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exportar resumen\n\nGuarda la tabla de resultados en CSV para compartir o analizar fuera del notebook.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "output_path = Path(\"experiment_summary.csv\")\nsummary_df.to_csv(output_path, index=False)\noutput_path\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}